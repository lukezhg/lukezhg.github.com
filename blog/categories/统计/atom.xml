<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 统计 | Chunlei's Blog]]></title>
  <link href="http://zhangchunlei.com/blog/categories/统计/atom.xml" rel="self"/>
  <link href="http://zhangchunlei.com/"/>
  <updated>2015-02-10T07:30:53+08:00</updated>
  <id>http://zhangchunlei.com/</id>
  <author>
    <name><![CDATA[chunlei]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Pick the right test]]></title>
    <link href="http://zhangchunlei.com/blog/2014/05/30/pick-the-right-test/"/>
    <updated>2014-05-30T14:14:00+08:00</updated>
    <id>http://zhangchunlei.com/blog/2014/05/30/pick-the-right-test</id>
    <content type="html"><![CDATA[<p>You have to pick a test to analysis your data. The question is which test is the right one. Well, it depends on your data and your hypothesis. Usually there are lot of choices, but only one solves your problem best.</p>

<p>Here are some suggestions for selecting test by the types of your variables.<br/>
Dependent and Independent Variables<br/>
Dependent Variable(DV) is what you are care of. Independent Variable(IV) is what you think may affect your dependent variable. Usually you can control your independent variable, but not depedent variable.</p>

<p>There are two basic types of variables:<br/>
Continuous Variables(e.g. height,temprature)<br/>
Discrete Variables(e.g. age, size, preferences)</p>

<p>if IV=[0,1],DV=[~] then T test<br/>
if IV=[0,1,2,],DV=[~] then one-way Anova or ANOCVA<br/>
if IV1=[0,1,2,], IV2=[0,1,2,], DV=[~] then General linear model<br/>
if IV1=[~], IV2[~],..., DV=[~], then Linear Regression &amp; Pearson<br/>
if IV=[0,1,...], DV=[0,1,...] then χ2<br/>
if IV=[~], DV=[0,1,...] then Logistic regression</p>

<p>For more details look at the following pictures:<br/>
<img src="https://raw.github.com/lukezhg/Freyja/master/choiceoftest.png" alt="Choices of Test" /><br/>
This picture comes from : <a href="http://www.socr.ucla.edu/Applets.dir/ChoiceOfTest.html">SOCR: Statistics Online Computational Resource</a></p>

<p>References:<br/>
<a href="http://www.unesco.org/webworld/idams/advguide/Chapt1_3.htm">Types of Variables</a><br/>
<a href="http://www.socr.ucla.edu/Applets.dir/ChoiceOfTest.html">SOCR: Choice of Test</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Multidimensional Scaling in R]]></title>
    <link href="http://zhangchunlei.com/blog/2014/05/29/mds-in-r/"/>
    <updated>2014-05-29T22:10:00+08:00</updated>
    <id>http://zhangchunlei.com/blog/2014/05/29/mds-in-r</id>
    <content type="html"><![CDATA[<p>I met this problem when I was analysing teachers' PCK concept maps. I wanted to divide them into several clusters that shared the similar structures(nodes and links).</p>

<p>One approch is plotting them in a 2d picture to see if there is any patterns. But before that, you have to do the Muultidimensional Scaling to get your (x,y) from your Distance Matrix.</p>

<p>Distance Matrix<br/>
<img src="https://raw.github.com/lukezhg/Freyja/master/distance-matrix.png" alt="Distance Matrix" /><br/>
How I got this matrix. Well, I just count them using my data. Of course, if can deside how you count them, which counts how much score, or you simply give them a number.</p>

<pre><code>
# 1) MDS 'cmdscale'
dm <- read.table("distance-matrix.txt")
mds1 = cmdscale(dm, k = 2)

# plot
plot(mds1[,1], mds1[,2], type = "n", xlab = "", ylab = "", axes = FALSE, main = "cmdscale (stats)")
text(mds1[,1], mds1[,2], labels(x)[[1]], cex = 0.9, xpd = TRUE)
</code></pre>


<p>This the x y data I got:<br/>
<img src="https://raw.github.com/lukezhg/Freyja/master/xy-data-mds.png" alt="Distance Matrix" /></p>

<p>I got this picture after plotting it out.<br/>
<img src="https://raw.github.com/lukezhg/Freyja/master/plot-mds.png" alt="Distance Matrix" /></p>

<p>Reference:<br/>
<a href="http://gastonsanchez.com/blog/how-to/2013/01/23/MDS-in-R.html">7 Functions to do Metric Multidimensional Scaling in R | Gaston Sanchez</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[R语言学习笔记]]></title>
    <link href="http://zhangchunlei.com/blog/2014/04/21/how-to-use-r/"/>
    <updated>2014-04-21T23:35:00+08:00</updated>
    <id>http://zhangchunlei.com/blog/2014/04/21/how-to-use-r</id>
    <content type="html"><![CDATA[<pre><code>
##week1
getwd() #获取当前工作目录
setwd(dir) #设定当前工作目录 也可以直接通过菜单界面设定
source("myfunction.R") #加入源之后才可以使用这个函数

##week2
#list
> a <- 1:10
> b <- c(1,3,4,5,8,9,11,14,15,18)
> b <- b[-2] #去除第二个元素
> c(1,2)*c(2,3)
[1] 2 6

> m <- c(1,2,3)
> mynames <- c("1-2","2-3","4-5")
> mytable <- setNames(m,mynames)#之后可以通过name来调用元素值
1-2 2-3 4-5 
  1   2   3 

> union(x, y) #并集
> intersect(x, y) #交集
> setdiff(x, y) #前者有后者没有的
> setequal(x, y) #元素是否全部相同
> is.element(el, set)
> unlist() #将层级list变为单层
> unique(x) # 获取非重复元素

# logical operation
x <- FALSE
y <- TRUE
! x
x & y
x && y
x | y
x || y
xor(x, y)

isTRUE(x)
cbind(a,b) #
rbind(a,b) #
length(a) #
class(a)
as.character()
as.logical()
as.numeric()
m <- matrix(a,nrow=2,ncol=5)
dim(m) #[1] 2 5
attributes(m) $dim [1] 2 5
m <- 1:10
dim(m) < c(2,5) #
f <- factor("yes","no","yes","yes")
table(f) # yes 3 no 1
x <- c()
x <- c(x,1)
mean() #求平均值
cor(a,b) #求相关系数
is.na()
is.nan()

## file 文件
read.table() #读取txt表格
read.csv() #读取csv文件
# 写文件略过row name
write.csv(data, "data.csv", row.names=FALSE)
# 同样写文件，当用空白替换"NA"
write.csv(data, "data.csv", row.names=FALSE, na="")
# 使用tabs略过row col name
write.table(data, "data.csv", sep="\t", row.names=FALSE, col.names=FALSE) 
参考[Cookbook for R » Writing data to a file](http://www.cookbook-r.com/Data_input_and_output/Writing_data_to_a_file/)
write.table(sim, "pcksim.csv", row.names=na,col.names=c("",na),sep = ",") # 可以写列名称

names()
x[!is.na(x)] # 去除缺失的数据
data <- data.frame(id=1:10,height=170:180)
data["id"] #返回局部data.frame
data["id"][!is.na(data["height"])] #返回height无缺失的id
data[[id]] #返回vector
paste("00","1",".csv",sep="") # 001.csv
paste(a,collapse="") # 将一个list连成一个长字符

for (i in 1:length(id)){
    s <- as.character(id[i])
    spre <- paste(rep("0",3-nchar(s)),collapse="") # use collapse join vector
    fn <- paste(directory,"/",spre,s,".csv",sep="") # use sep to join string
    data <- read.csv(fn)
}

if(length(x)>0){print(x)}else{}

##Week3
x <- list(a=1:5,b=rnorm(10))
lapply(x,mean)#$a[1] 3  $b[1] 0.03937816
如果x不是一个list,那么它将被自动转换成list,相当于使用as.list()函数
x <- list(a=matrix(1:4,2,2),b=matrix(1:6,3,2))
lapply(x,function(elt)elt[,1]) #$a[1] 1 2  $b[1] 1 2 3
lapply(x,function(elt) strsplit(elt,"-")) #对每个元素使用“-”进行拆分
sapply(x,mean) #会自动简化结果，返回一个vector或者matrix，不能简化返回list
x <- matrix(rnorm(200),20,10)
apply(x,2,mean) #返回10列的均值
apply(x,1,sum) #返回每行的总和共20个
a <- array(rnorm(2*2*10),c(2,2,10)) #三维数组
apply(a,c(1,2),mean)
          [,1]      [,2]
[1,] 0.1888774 0.5517366
[2,] 0.2667046 0.2412767

str() #Compactly Display the Structure of an Arbitrary R Object
str(tapply)
> x <- c(rnorm(10),runif(10),rnorm(10,1))
> f <-gl(3,10) #生成标签 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3
> tapply(x,f,mean) #具有相同标签的数值的平均值
with(mtcars, tapply(mpg, cyl, mean))# mtcars is a data.frame with cyl and mpg

> split(x,f) #利用标签将list进行拆分
> s <- split(airdata, airdata$Month)
> lapply(s,function(x) colMeans(x[,c("Ozone","Solar.R","Wind")]))
split(x,list(f1,f2),drop=TRUE) # empty levels can be dropped

## 字符串 string
> as.character()
> strsplit("1-2,3-4,5-6",",") #对字符串进行拆分
> strsplit("1-2,3-4,5-6",",")[[1]][1] #返回 "1-2"
> paste(rev(strsplit("abc", split = "")[[1]]), collapse = "") #反转字符串
> grep("bcd", "abcd") # return 1 # 
> regexpr("bcd","aabcd")
[1] 3
attr(,"match.length")
[1] 3
attr(,"useBytes")
[1] TRUE

> gregexpr("1","1-2,2-3,1-5") #返回所有匹配位置 也能利用length获取匹配数目
> length(gregexpr("1","1-2,2-3,1-5,1-9,1-8")[[1]]) #返回4


> mapply(rep,1:4,4:1)
[[1]]
[1] 1 1 1 1

[[2]]
[1] 2 2 2

[[3]]
[1] 3 3

[[4]]
[1] 4

## debug
traceback() # print out where error occurs,else do nothing
debug() # step through one line at a time
browser() #suspend the execution wherever called and put it in debug mode
trace() # insert debug code in specific place

#Play with the iris data
library(datasets)
data(iris)

#Play with $
> x <- makeCacheMatrix()
> x
$set
function (y) 
{
    x <<- y
    inv <<- NULL
}

$get
function () 
x

$setinverse
function (inverse) 
inv <<- inverse

$getinverse
function () 
inv

> x$set(matrix(rnorm(9),3,3))
> x$get()

> x$set(a)
> x$get()
            [,1]      [,2]       [,3]
[1,] -0.50184759 -0.751659 -2.1276852
[2,]  0.37466264 -1.448643  1.1807655
[3,] -0.06093845  0.740338 -0.8508723
> cacheSolve(x)
           [,1]       [,2]       [,3]
[1,] -0.4668678  2.8847043  5.1705844
[2,] -0.3214999 -0.3872940  0.2664879
[3,] -0.2462983 -0.5435808 -1.3137063

#week4
#统计
str(str)# see the structure of a object
m <- matrix(rnorm(100),10,10)
str(m)
str(lm)
str(ls)
x <- rnorm(100,2,4)
summary(x) #摘要
table(x) #返回频数表
str(x)

#作图
hist(rnorm(1000,2,4))
hist(BMI, breaks=20, main="Breaks=20")
hist(BMI, breaks=seq(17,32,by=3), main="Breaks is vector of breakpoints")
seq(0, 1, length.out = 10) #在一定范围产生制定数目的序列
plot(x,y)

#拟合
> set.seed(20)  #进而可以再次产生同样的随机数 方便别人重复模拟
> x <- rnorm(100)
> e <-rnorm(100,0,2)
> y <- 0.5+2*x+e
> sumary(y)

#评估运算时间 user time（cpu 时间消耗）elipsed time(流逝壁钟时间)
> system.time(readLines("http://www.jhsph.edu"))

#就
make.Negloglik <- function(data, fixed=c(FALSE,FALSE)){
    params<- fixed
    function(p){
        params[!fixed] <- p
        mu <- params[1]
        sigma <- params[2]
        a <- -0.5*length(data)*log(2*pi*sigma^2)
        b <- -0.5*sum((data-mu)^2)/(sigma^2)
        -(a+b)
    }
}
set.seed(1)
normals <- rnorm(100,1,2)
nLL <- make.Negloglik(normals)
optim(c(mu=0,sigma=1),nLL)$par

> # Fixing sigma=2
> nLL <- make.Negloglik(normals,c(FALSE,2))
> optimize(nLL,c(-1,3))$minimum
[1] 1.217775
> optimize(nLL,c(-1,2))$minimum
[1] 1.217775
> #Fixing u=1
> nLL <- make.Negloglik(normals,c(1,FALSE))
> optimize(nLL,c(1e-6,10))$minimum
[1] 1.800596
> nLL <- make.Negloglik(normals,c(1,FALSE))
> x <- seq(1.7,1.9,len=100)
> y <- sapply(x,nLL)
> plot (x,exp(-(y-min(y))),type="l")
> nLL <- make.Negloglik(normals,c(FALSE,2))
> x <- seq(0.5,1.5,len=100)
> y <-sapply(x,nLL)
> plot(x,exp(-(y-min(y))),type="l")


Assignment 3
#在R中有的时候表达方式是不一样的，比如
引用某个变量的子变量用
time$year #而不是time.year

##常犯错误
该使用[]的时候错误的使用了（），特别是在操作data.frame的时候
该使用[[]]错误的使用了[]，前者可以变为向量，后者还是data.frame
忘记了使用%%进行
判断是否包含 
"a" %in% c("a","b","c")
表示数组相乘用 
%*%

如何使用Order
> (ii <- order(x <- c(1,1,3:1,1:4,3), y <- c(9,9:1), z <- c(2,1:9)))
 [1]  6  5  2  1  7  4 10  8  3  9
 > rbind(x, y, z)[,ii] # shows the reordering (ties via 2nd & 3rd arg)
  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
x    1    1    1    1    2    2    3    3    3     4
y    5    6    9    9    4    7    1    3    8     2
z    5    4    1    2    6    3    9    7    2     8


</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Repeated Measures Correlation Mixed Model]]></title>
    <link href="http://zhangchunlei.com/blog/2014/04/17/repeated-measures-correlation-mixed-model/"/>
    <updated>2014-04-17T09:02:00+08:00</updated>
    <id>http://zhangchunlei.com/blog/2014/04/17/repeated-measures-correlation-mixed-model</id>
    <content type="html"><![CDATA[<p>How to do it?<br/>
Analysis==>Mixed Model, then you got diaogue 1 as follows. ID shuold be put be included as subjects. Year should be included as repeated, cause I measured multiple times by different year.
<img src="https://raw.github.com/lukezhg/Freyja/master/mixed-model-d1.png" alt="Mixed Model Dialouge 1" /><br/>
The most hard decision to make is which Repeated Covaricance Type to select, there are a lot of choices, the most used ones are:<br/>
1. Unstructured: you have no idea of the correlation of repeated measures, SPSS we estimate all the correlation independantly (r12,r13,r23,r14,r24,r34).<br/>
2. Scaled Identity: no correlations at all, r=0.<br/>
3. Compound Symmetry: all correlations equals r.<br/>
4. AR1: (r, r<sup>2,</sup> R<sup>3)<br/></sup>
<img src="https://raw.github.com/lukezhg/Freyja/master/mixed-model-AR1.png" alt="Auto Regression 1" /><br/>
5. Toeplitz:(a,a,a,b,b,c)<br/>
<img src="https://raw.github.com/lukezhg/Freyja/master/mixed-model-toepliztz.png" alt="Toeplitz" /><br/>
6. Another choice is "diagonal" which assumes no correlation between the random effects.</p>

<p>Then you determing the fixed effect variable:<br/>
<img src="https://raw.github.com/lukezhg/Freyja/master/mixed-model-d2.png" alt="Mixed Model Dialouge 2" /></p>

<p>And the random effect variable:<br/>
<img src="https://raw.github.com/lukezhg/Freyja/master/mixed-model-d3.png" alt="Mixed Model Dialouge 3" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[多次重测数据的方差分析]]></title>
    <link href="http://zhangchunlei.com/blog/2014/04/14/repeated-measures-anova/"/>
    <updated>2014-04-14T13:57:00+08:00</updated>
    <id>http://zhangchunlei.com/blog/2014/04/14/repeated-measures-anova</id>
    <content type="html"><![CDATA[<p>统计学比较抽象，需要通过例子加以学习。</p>

<p>比如第一型错误（Type I error）和第二型错误（Type II error）。比如验证一个人的智商是否超常（极高或者极低），我们知道95%人群的智商在70-130之间，一般成正态分布。我们会做出一个零假设H0，假定某人的智商是正常的。然后用某种评估方法测量一个人的智商结果是145，超出正常范围，这样我们就否定零假设，认为该人智商是超常的。</p>

<p>但是我们的评估方法不一定准确，所以不可避免的会犯错误。第一型错误是，该人智商属于正常范围（接受H0），但是的统计评测显示的却是超常（拒绝H0）。第二型错误是，该人智商明明超常（拒绝H0），结果我们统计评测显示该人智商正常（接受H0）。简单说也就是假阳性和假阴性，或者假检出和假包含。</p>

<p>分析数据就像是在森林里寻宝，每次进入森林都很不容易，需要花很多时间，非常容易迷失方向，面对无数的表格和无数的可能分析方法不知所措。而且好不容要到的路径，下次再想用却忘记了，需要花费很多实践才能再次找到。于是很多人做了很多无用的功，就是在原地打转。为了避免这样的情形发生，分析数据时一定要写分析记录和报告，否则即便是文件夹里有无数的报表，也忘记当初这个报表是干什么的了。</p>

<p>如何使用SPSS绘制重复测量的误差条图？答案是可以使用SPSS的graph builder，不过我在用的过程中发现一个很低级的错误。我搜索到了一个视频教程<a href="http://vimeo.com/49231146">Create a clustered bar or line chart of means for repeated measures data (with optional error bars) on Vimeo</a>。其实原理很简单，就是把多个变量同时拉到纵轴。但是我拉的时候，系统却不容许，也不报错。晕。后来发现，原来这些变量必须是scale类型，而不能是normal。全部粘贴覆盖为scale之后，问题解决。不提示为什么不能拖拽，这是SPSS设计上的缺陷。其实类似的问题还有，当你使用Multiple Imputation(MI)处理缺失数据的时候，如果变量类型不是scale，SPSS也会拒绝执行MI，这也是在网上查了才解决的。因此，在使用SPSS的时候一定要小心的变量类型，尽量最开始就设好。</p>

<p>下面进入正题，如何进行重复测量的方差检验。其实如何做并不难，难的是如何解释Spss产生的一大推报表，并选择有用的信息进行报告。首先这样的检测一般可以分为两类：One Way Anova以及Two-way Anova。前者是单因素实验设计，后者是双因素实验设计（如V1（2）× V2（3）交叉设计，共有6组）。两者的区别以后再解释。</p>

<p>此外还有组间变量（比如不同的处理、教学方法、培训类型）和协变量（性别、教育程度）等。</p>

<p>这种统计方法往往可以让你检测是否存在交叉效应。</p>

<p>在报告的时候，SPSS同时给你multivariate test和Univariate test的结果。到底报告那个呢？取决于Mauchly's Test of Sphericity，如果sig&lt;0.05那么，说明球度（sphericity）假设不成立，那么就要看修正后的Univerite test的结果。Spss提供多个修正结果，根据Epsilon-Greenhouse-Geisser的值是否大于0.75进行不同的选择，如果大于则选择Huynh-Feldt修正的结果，否则则选Greenhouse结果。<br/>
另外就是如何报告F检验的结果。通常报告的格式如下F(自由度，误差自由度)=F值，p=sig，n=total。这些值在哪里呢？看下图。<br/>
<img src="https://raw.github.com/lukezhg/Freyja/master/f-test-report.png" alt="F test report" /></p>

<p>那么如何报告One way Anova的结果呢？<br/>
<a href="https://raw.github.com/lukezhg/Freyja/master/one-way-anova.png">One way Anova</a><br/>
APA报告格式为：F(3,165)=2.836 p=0.04&lt;.05<br/>
其中3为组间自由度（组数4-1），165为组内自由度（样本总数169-组数4）。所以看到这个结果，就知道共分4组，样本总量为169（3+165+1）。F值为2.836，p值为0.04，达到了显著水平。</p>
]]></content>
  </entry>
  
</feed>
